## Introduction
Large Language Models contain vast amounts of useful knowledge but they are also unreliable and often wrong. Good prompts are essential for getting correct information and building context into the prompt results in better performance..

In this project we propose a method of concept decomposition and a recursive algorithm with prompts to generate a Knowledge Base (KB) on an arbitrary input concept (IC). By storing these outputs in a python dictionary we implement an external memory that allows for more inclusion of additional specific context when generating prompts. By algorithmically breaking down the IC into a set of components using a series of prompts we are able to focus on generating higher quality prompts for the specific sub-concept. Additional requests to summarize provide concept and sub-concept descriptions.

With this implementation we create and deploy RichardsAI University - home to 100 (in progress) algorithmically generated courses.

Richards AI University (RAU) is a collection of output generated by recursively querying a large language model (LLM) for information on 10 high level fields of study(FOS) and their sub(sub) fields (SSF).  Below we see the sub-sub fields of Medicine:Biochemistry are:

['Cell Biology', 'Genetics', 'Microbiology', 'Neurobiology', 'Ecology', 'Evolutionary Biology', 'Developmental Biology', 'Biochemistry', 'Biophysics', 'Biotechnology']

The source code to generate the structure below and the associated course content (and publish the content to a moodle server) is freely available.  



## FAQ
What is the purpose of RAU?
The purpose of RAU is to explore a LLM’s knowledge on a set of topics and identify when it is consistently incorrect (and inconsistently correct).  By generating this content and making it generally available it is my hope that a community of subject matter experts will contribute to the validation and curation of the content in a way that is beneficial for everyone.

It is my intention to track individual contributions (feedback/corrections to course content) and use the feedback to drive improvements to future prompting strategies as well as the generation of content for fine tuning a model with additional content.  The corrections and feedback will also be made publicly available so that it can be used as other researchers see fit.

What data do you collect and why?
We collect data about you when you join the community.  We are hoping to use this information to track your contribution to the “Reinforcement Learning through Human Feedback” portion of the project.  This is future work but the idea is to track and reward contribution (improvement) to future models and provide rewards in the form of tokens (coins) that can be used as a currency to query the model or traded for other types of coin (eg. bitcoin).



## Privacy Statement
https://rau.richards.ai/privacy.html



##Terms of Service
https://rau.richards.ai/tos.txt

